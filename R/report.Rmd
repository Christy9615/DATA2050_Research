---
title: "Predicting TB Medication Adherence Using a Risk Score Model"
date: "November 2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(knitr)
library(xtable)
library(glmnet)
source('risk.R')

```

# Data Preprocessing 

In order to create a risk score model, we must have a binary outcome with integer (or near-integer) covariates. The raw data includes two continuous outcome variables -- `PCTadherence` and `PCT_adherence_sensi`. Both measure the percentage of doses taken on time, the former calculating this by recorded number of doses missed and the latter calculating this by treatment length and expected treatment length. To make these outcomes binary, we'll need to define a cutoff above which we'll consider the patient "adherent". Ideally, there should be approximately equal numbers of patients in the "adherent" and "non-adherent" groups. Table 1 explores the class imbalance when using a cutoff of 100% (where a patient must have 100% adherence to be considered "adherent"). We observe better balance when using the `PCTadherence_sensi` variable, with 39.1% of patients classified as "non-adherent" (n = 110) and 60.2% of patients classified as "adherent" (n = 154). We will dichotomize the `PCTadherence_sensi` variable and move forward with this as the outcome of interest. 


```{r}
tb_df <- read.csv("/Users/hannaheglinton/Library/CloudStorage/OneDrive-BrownUniversity/Thesis/Data/Peru_TB_data.csv")

data.frame(some_missed = c(sum(tb_df$PCTadherence < 100, na.rm = TRUE),
                           sum(tb_df$PCTadherence_sensi < 100, na.rm  = TRUE)),
           none_missed = c(sum(tb_df$PCTadherence == 100, na.rm = TRUE),
                           sum(tb_df$PCTadherence_sensi == 100, na.rm  = TRUE)),
           missing = c(sum(is.na(tb_df$PCTadherence)),
                       sum(is.na(tb_df$PCTadherence_sensi))),
           row.names = c("PCTadherence", "PCTadherence_sensi"))  %>%
  kable(col.names = c("<100% Adherence", "100% Adherence", "Missing"),
        caption = "Patient Count by Dichotomized Outcome")
  
  
```

The raw data includes 64 potential covariates. We dropped `PTID2`, `hunger_freq`, `health_ctr`, and `post_tb_pt_work` because they were either not listed in the data dictionary or were listed with a note that they should not be included in the model. We summarized the family support, evaluation of health services, motivation, and TB disinformation variables by taking the median value for each category. 

The data dictionary noted that `pills` variable was coded as 1 for 0-3 pills, 2 for 4-6 pills, 3 for 7-9 pills, 4 for 10-11 pills, and 5 for 12+ pills. However, the values in the data were 0.25, 0.50, 0.75, and 1.00. We converted this variable to the scale in the data dictionary by multiplying each value by 4. However, note that this results in no values of 5 in the cleaned data (indicating no subjects taking 12+ pills). The `adr_freq` variable also needed to be multiplied by 4 for the same reason. In this variable's case, all values listed in the data dictionary (0, 1, 2, 3, and 4) are present in the transformed data. 

There were four continuous variables that were converted to categorical using the following cutoffs (selected based on class balance): 

* `age_BLchart`: <16 years, 16-17 years, 18+ years
* `audit_cat`: 0, >0 
* `ace_cat`: 0, 1, >1 
* `tx_mos_cat`: $\le6$ months, >6 months

The categorical variables `ram` and `regular_drug` were dropped because they each had only 5 patients responding with "yes". The variables `edu_level_mom` and `edu_level_dad` could potentially be combined, but they were not included in the data dictionary so they were dropped. The `monitor1` variable was also dropped because it was not clear how to relevel it. 

The preprocessed dataset contains 41 covariates, which includes the dummy variables for categorical covariates. 


```{r}
source('tb_preprocessing.R')
df <- tb_preprocessing(tb_df)

#set.seed(5)
#train_index <- sample(1:nrow(df), 0.80*nrow(df), replace = FALSE)
#val_index <- seq(1:nrow(df))[-train_index]

```



# Model Fitting 

Out model-fitting algorithm uses a parameter $\lambda_0$ that penalizes nonzero coefficients. In other words, a higher value of $\lambda_0$ will result in fewer covariates being included in the risk score model. To determine the best value for $\lambda_0$, we run cross-validation. Figure 1 plots the mean model deviance for a range of potential $\lambda_0$ values. The best fitting model will have the lowest deviance. The numbers at the top of the plot show the number of nonzero coefficients in the model fit with each value of $\lambda_0$. We can see that the $\lambda_0$ producing the lowest mean deviance results in a model with 4 features. Note that due to the relatively small size of this data set, these cross validation findings aren't robust. In fact, changing the randomization seed will cause models with different numbers of nonzero coefficients to have the lowest deviance, including models with no nonzero coefficients. 

```{r, out.width="75%", fig.align = 'center'}
#X <- as.matrix(df[train_index, -ncol(df)])
#y <- df[train_index, ncol(df)]

X <- as.matrix(df[,-ncol(df)])
y <- df[,ncol(df)]

# CV
# get folds
folds <- stratify_folds(y, nfolds = 5, seed = 5)

cv_results <- cv_risk_mod(X, y, foldids = folds, a = -5, b = 5)

plot(cv_results, lambda_text = FALSE) + 
  labs(title = "Figure 1. Cross Validation Results")


```

The four features included in the risk score model with $\lambda_0 = 0.02$ were `lives_w_mom`, `health_svc_median`, `motiv_median`, `tx_mos_cat_>6mos`. The score card is shown in Table 2, where the total score for a patient can be calculated by multiplying each variable response by the number of points shown on the right. For example, if a patient lived with their mom, had a median health services score of 4, had a median motivation score of 3.5, and their treatment lasted 3 months, their total score would be $1(-7) + 4(1) + 3.5(-2) + 0(10) = -10$. Table 3 can then be used to map this score to its associated risk of non-adherence. For this example patient, their risk would be 36%. 


```{r}
mod <- risk_mod(X, y, lambda0 = cv_results$lambda_min, a = -5, b = 5)

```

```{r}

data.frame(mod$model_card, row.names = c("Lives with Mom (0/1)", 
                    "Pills Value (1-5)",
                    "Accompanied by Family (1-5)",
                    "Family Dislikes Friends (1-5)",
                    "Feels Ashamed in Health Center (1-5)",
                    "Has Never Had Covid (0/1)",
                    "Median Motivation Score (1-5)",
                    "Treatment >6 months (0/1)")) %>%
kable(caption = "Score Card" )

score_range <- seq(-14, 18, 2)

data.frame(Score = as.character(score_range), 
           Risk = as.character(round(get_risk(mod, score_range), 2))) %>%
  t() %>%
  kable(align = "c", caption = "Score-Risk Map")

```

Figure 2 visualizes the logistic regression curve of this model. The observed scores from this dataset are plotted as points along the curve. The blue crosses represent the lowest and highest possible scores given our model. We can see that these do not correspond with 0% and 100% risk, respectively. In fact, the lowest risk that this model can predict is 23% and the highest is 82%. 


```{r, out.width = "75%", fig.align='center'}


predict_df <- data.frame(score = predict(mod, type = "score"),
                         response = predict(mod, type = "response"))

ggplot() + 
  geom_point(data = predict_df, aes(x = score, y = response), size = 1) +
  geom_function(data = data.frame(x = seq(-35,30)), aes(x), 
                fun = function(x) get_risk(mod, x)) + 
  labs(x = "Score", y = "Risk", title = "Figure 2. Risk Score Model") +
  scale_x_continuous(breaks = seq(-60, 50, 10)) + 
  scale_y_continuous(breaks = seq(0, 1, 0.10)) + 
  #geom_point(aes(x = -14, y = get_risk(mod, -14)), color = "blue", 
  #           shape = 3, size = 3) + 
  #geom_point(aes(x = 18, y = get_risk(mod, 18)), color = "blue", 
  #           shape = 3, size = 3) + 

  theme_bw()
```




# Model Evaluation 


```{r, eval = FALSE}
# Logistic Regression 
logistic_mod <- glm(y ~ X[,-1], family = binomial())


# Rounded Logistic Regression 
rounded_coef <- round(coef(logistic_mod),0)




# Lasso 
lasso_cv <- cv.glmnet(X[,-1], y, nfolds = 5, foldid = folds, 
                         alpha = 1, family = "binomial") 
lasso_coef <- coef(lasso_cv, lambda = exp(-4))


# Rounded Lasso 



# Risk Score 
```


\pagebreak

# EDA Appendix


\pagebreak

# Code Appendix



```{r  ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, include=TRUE}

```


