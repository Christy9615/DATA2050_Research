---
title: "Predicting TB Medication Adherence Using a Risk Score Model"
date: "February 2024"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#devtools::install_github("hjeglinton/riskscores", build_vignettes = TRUE)

library(knitr)
library(xtable)
library(glmnet)
library(pROC)
library(tableone)
library(glmnet)
library(caret)
library(gridExtra)
#library(kableExtra)
library(gtsummary)
library(riskscores)
source('risk.R')

```

# Data Preprocessing 



```{r}
raw_data <- read.csv("../data/Peru_TB_data.csv") 

```



This study included 249 individual participants. Since some participants had multiple regimens, the raw data contained 266 regimens in total. For this analysis we considered only the first regimen for each participant. After removing the subsequent regimens, the data contained one row for each participant (n = 249).

The raw data included 64 potential covariates. We dropped `PTID2`, `hunger_freq`, `health_ctr`, and `post_tb_pt_work` because they were either not listed in the data dictionary or were listed with a note that they should not be included in the model. We summarized the family support, evaluation of health services, motivation, and TB disinformation variables by taking the median value for each category. 

The data dictionary noted that `pills` variable was coded as 1 for 0-3 pills, 2 for 4-6 pills, 3 for 7-9 pills, 4 for 10-11 pills, and 5 for 12+ pills. However, the values in the data were 0.25, 0.50, 0.75, and 1.00. We converted this variable to the scale in the data dictionary by multiplying each value by 4. Note that this results in no values of 5 in the cleaned data (indicating no subjects taking 12+ pills). The `adr_freq` variable also needed to be multiplied by 4 for the same reason. In this variable's case, all values listed in the data dictionary (0, 1, 2, 3, and 4) are present in the transformed data. 

Distributions of the continuous covariates are visualized in the appendix. There were seven continuous variables that were converted to categorical using the following cutoffs (selected based on class balance or background knowledge): 

* `age_BLchart`: <16 years, 16-17 years, 18+ years
* `audit_cat`: 0, >0 
* `ace_cat`: 0, 1, >1 
* `tx_mos_cat`: $\le6$ months, >6 months
* `self_eff_cat`: score of $\le12$, >12
* `stig_tot_cat`: score of $\le30$, >30
* `phq9_tot_cat`: score of $\le10$, >10


The categorical variables `ram` and `regular_drug` were dropped because they each had only 5 patients responding with "yes". The variables `edu_level_mom` and `edu_level_dad` could potentially be combined, but they contained many "I don't know" responses so were also dropped. 

After processing, the dataset contained 32 covariates (listed in the Appendix). We excluded 37 patients with missing covariate data, leaving us with 212 observations. 

In order to create a risk score model, we must have a binary outcome with integer (or near-integer) covariates. Here, we are interested in medication adherence, which is a continuous variable that measures the percentage of doses taken on time. Although a 90% cutoff would align with established TB research, our dataset includes only 17 non-adherent participants without missing data. To reduce class imbalance, we defined a `PCTadherence_sensi` cutoff of 90% above which we considered the patient "adherent". Using this cutoff, we observe 86 participants classified as "non-adherent" (41%) and 124 participants classified as "adherent" (59%). 



```{r}
source('tb_preprocessing.R')
tb_df <- tb_preprocessing(raw_data) 
tb_mat <- tb_as_matrix(tb_df)

```



# Model Fitting 

Out model-fitting algorithm uses a parameter $\lambda_0$ that penalizes nonzero coefficients. In other words, a higher value of $\lambda_0$ will result in fewer covariates being included in the risk score model. To determine the best value for $\lambda_0$, we ran cross-validation. Figure 1 plots the mean model deviance for a range of potential $\lambda_0$ values. The best fitting model will have the lowest deviance (shown in red). The numbers at the top of the plot show the number of nonzero coefficients in the model fit with each value of $\lambda_0$. We can see that the "best" model for these data has five nonzero coefficients and uses a $\lambda_0$ value of $exp(-4.7)$.


```{r, out.width="75%", fig.align = 'center', warning = FALSE}

X <- as.matrix(tb_mat[,-ncol(tb_mat)])
y <- tb_mat[,ncol(tb_mat)]

# CV
# get folds
folds <- stratify_folds(y, nfolds = 5, seed = 1)

cv_results <- cv_risk_mod(X, y, foldids = folds, a = -10, b = 10, nlambda = 25)

plot(cv_results, lambda_text = FALSE) + 
  labs(title = "Figure 1. Cross Validation Results")


```

The score card for the model with $\lambda_0 = exp(-4.7)$ is presented in Table 1. A patient's total score can be calculated by multiplying each variable's response by the number of points shown on the right and then adding the points together. For example, if a patient's responses to the variables listed in Table 1 were 1, 1, 1, 1, 5, 3, and 4, respectively, their total score would be $1(10) + 1(3) + 1(-4) + 1(6) + 5(-1) + 3(-2) + 4(-2)  = -4$. Table 2 can then be used to map this score to its associated risk of non-adherence. For this example patient, their risk of non-adherence would be 16%. 


```{r, warning = FALSE}
lambda0 <- exp(-4.7)

mod <- risk_mod(X, y, lambda0 = lambda0, a = -10, b = 10)

```

```{r}

data.frame(mod$model_card, row.names = c("Daily Cont (0/1)", 
                    "Has Never Had Covid (0/1)",
                    "In-Person DOT Only (0/1)",
                    "In-Person DOT and Family Supervision (0/1)",
                    "Total Stigma Score >30 (0/1)")) %>%
  
  kable(caption = "Score Card" , 
                  booktabs = TRUE, linesep = "")  #%>% 
  #kableExtra::kable_styling(latex_options = c("HOLD_position"), font_size = 10)

score_range <- seq(-6, 8)

data.frame(Score = as.character(score_range), 
           Risk = as.character(round(get_risk(mod, score_range), 2))) %>%
 # t() %>%

  kable(caption = "Score-Risk Map" , 
                 booktabs = TRUE)#  %>% 
  #kableExtra::kable_styling(latex_options = c("HOLD_position"), font_size = 8) %>%
  #row_spec(row = 1, hline_after = TRUE)

```

Figure 2 visualizes the logistic regression curve of this model. The observed scores from this dataset are plotted as points along the curve. 


```{r, out.width = "75%", fig.align='center'}


predict_df <- data.frame(score = predict(mod, type = "score"),
                         response = predict(mod, type = "response"))

ggplot() + 
  geom_point(data = predict_df, aes(x = score, y = response), size = 1) +
  geom_function(data = data.frame(x = seq(-8,10)), aes(x), 
                fun = function(x) get_risk(mod, x)) + 
  labs(x = "Score", y = "Risk", title = "Figure 2. Risk Score Model") +
  #scale_x_continuous(breaks = seq(-60, 50, 10)) + 
  #scale_y_continuous(breaks = seq(0, 1, 0.10)) + 
  #geom_point(aes(x = -14, y = get_risk(mod, -14)), color = "blue", 
  #           shape = 3, size = 3) + 
  #geom_point(aes(x = 18, y = get_risk(mod, 18)), color = "blue", 
  #           shape = 3, size = 3) + 

  theme_bw()
```






# Model Evaluation 

Figure 3 plots the distribution of scores by adherence outcome. We can see that those with lower scores tended to be adherent while those with higher score tended to be non-adherent. 


```{r, out.width = "80%", fig.align='center', message = FALSE}
# Figure 3
ggplot() + 
  geom_histogram(aes(x = predict_df$score, fill = factor(tb_df$adherence_outcome)),
                 alpha = 0.8, color = "white") + 
  labs(x = "Score", y = "Count", fill = "", 
       title = "Figure 3. Distribution of Scores by Outcome") + 
  scale_fill_discrete(labels = c("Adherence", "Non-Adherence")) + 
  theme_bw() + 
  theme(legend.position = "bottom")
```
We can also visualize the relationship between score and probability of adherence as in Figure 4. The grey points indicate the observed probability of adherence for each observed score. The orange line indicates the predicted probability of adherence associated with each score. We can see that this line tends to overestimate the probability of non-adherence. 

```{r, out.width = "80%", fig.align='center'}
# Figure 4
range_scores <- range(predict_df$score)
all_scores <- unique(predict_df$score)
vals <- mod$gamma*(mod$beta[1]+range_scores)
probs <- exp(vals)/(1+exp(vals))
props <- tb_df %>% 
  mutate(rnd_scores = floor(predict_df$score)) %>%
  group_by(rnd_scores) %>% 
  summarize(prop = sum(adherence_outcome)/n()) 

ggplot() + 
  geom_line(aes(x=range_scores, y=probs, color = "Predicted")) + 
  geom_point(aes(x=props$rnd_scores,y=props$prop, color = "Observed")) + 
  scale_color_manual(name = "", 
                     breaks = c("Predicted", "Observed"),
                     values = c("Predicted" = "#d95f02", "Observed" = "#5A5A5A")) + 
  labs(x="Score", y="Predicted or Observed Probability",
       title = "Figure 4. Predicted vs. Observed Probabilities per Score") + 
  theme_bw() + 
  theme(legend.position = "bottom")

```

We compared the performance of our risk score model to logistic regression, lasso regression, and rounded logistic regression. The coefficients for each of these models are reported in Table 3. As expected, the logistic regression model assigns a non-zero coefficient value to each covariate, while the lasso model shrinks many coefficients to zero. 

```{r, include = FALSE}

coef_vals <- matrix(0, ncol=4, nrow=ncol(X)-1)

# risk model prediction
coef_vals[, 1] <- coef(mod)[-1]
risk_probs <- predict_df$response
risk_pred <- as.factor(ifelse(risk_probs < 0.5, 0, 1))

# glm prediction
glm_mod <- glm(y~X-1, family = "binomial")
coef_vals[, 2] <- coef(glm_mod)[-1]
glm_probs<- predict(glm_mod, type="response")
glm_pred <- as.factor(ifelse(glm_probs < 0.5, 0, 1))

# lasso prediction
lasso_res <- cv.glmnet(x=X[,-1], y=y, alpha=1)
lasso_mod <- glmnet(x=X[,-1], y=y, lambda=lasso_res$lambda.min, alpha=1)
coef_vals[,3] <- coef(lasso_mod)[-1]
lasso_probs <- as.vector(predict(lasso_mod, newx=X[,-1]))
lasso_pred <- as.factor(ifelse(lasso_probs < 0.5, 0, 1))

# rounded logistic
coef_vals[,4] <- round(coef_vals[,2] / abs(median(coef_vals[,2])),0)
x_vars <- tb_mat[,-c(1,ncol(tb_mat))]
round_score <- x_vars %*% coef_vals[,4]
mod_round <- glm(tb_mat[,ncol(tb_mat)] ~ round_score, family = "binomial")
round_probs <- predict(mod_round, type="response")
round_pred <- as.factor(ifelse(round_probs < 0.5, 0, 1))


# discrimination
risk_roc <- roc(factor(tb_df$adherence_outcome), risk_probs)
glm_roc <- roc(factor(tb_df$adherence_outcome), glm_probs)
lasso_roc <- roc(factor(tb_df$adherence_outcome), lasso_probs)
round_roc <- roc(factor(tb_df$adherence_outcome), round_probs)


```

Performance metrics for these models are reported in Table 4. Overall, we observed that variable selection reduced the performance of the models, as the logistic and rounded regression models had AUC values above 0.97, while the risk score and lasso models had AUC values of 0.831 and 0.819. However, these metrics were evaluated using the training data, where the logistic and rounded logistic models were likely highly overfit. Rerunning these models on a validation data set would help estimate their true performance. 


```{r}
data.frame(coef_vals, 
           row.names = dimnames(X)[[2]][-1]) %>%
  kable(digits = 3, col.names = c("Risk", "Logistic", "Lasso", "Rounded"),
      booktabs = T, caption = "Model Coefficients") #%>%

  #kableExtra::kable_styling(font_size = 9, 
                           # latex_options = c("repeat_header", "HOLD_position"))
```


```{r}
rbind(coords(risk_roc, "best"), coords(glm_roc, "best"), 
      coords(lasso_roc, "best"), coords(round_roc, "best")) %>%
  data.frame(auc = c(risk_roc$auc[[1]], glm_roc$auc[[1]], lasso_roc$auc[[1]],
           round_roc$auc[[1]]), 
           row.names = c("Risk", "Logistic", "Lasso", "Rounded")) %>%
  kable(digit = 3, 
      col.names = c("Threshold", "Specificity", "Sensitivity", "AUC"),
      caption = "Model Performance",
      booktabs = T)# %>%

 # kableExtra::kable_styling(font_size = 9, 
                         #   latex_options = c("repeat_header", "HOLD_position"))
```

For our risk score model, a threshold of 32.3% risk resulted in the optimal performance. This probability corresponds to a score of between -1 and 0. 



# Conclusion

Despite a relatively small sample size, our example model output made sense intuitively. Our model identified higher stigma and having to take the medication in-person at a clinic as factors increasing the risk of non-adherence. Having never had COVID and having family supervision of treatment decreased the risk of non-adherence. Although the performance of this model should be validated with new data, it may still help with identifying which factors are the most important in predicting non-adherence. 


\pagebreak

# EDA Appendix



### Figure A1. Outcome Distributions
```{r, warning = FALSE, message = FALSE, fig.height = 3}
# outcome distributions
p1 <- ggplot(raw_data) +
  geom_histogram(aes(x = PCTadherence)) + 
  lims(y = c(0, 210)) + 
  theme_bw()

p2 <- ggplot(raw_data) +
  geom_histogram(aes(x = PCTadherence_sensi)) + 
  lims(y = c(0, 210)) + 
  theme_bw()

grid.arrange(p1, p2, ncol = 2)
```

### Figure A2. Continuous Covariate Distributions

```{r, warning = FALSE, message = FALSE, fig.height = 5}
p3 <- ggplot(raw_data) +
  geom_histogram(aes(x = self_eff)) + 
  theme_bw()

p4 <- ggplot(raw_data) +
  geom_histogram(aes(x = tx_mos)) + 
  theme_bw()

p5 <- ggplot(raw_data) +
  geom_histogram(aes(x = audit_tot)) + 
  theme_bw()

p6 <- ggplot(raw_data) +
  geom_histogram(aes(x = stig_tot)) + 
  theme_bw()

p7 <- ggplot(raw_data) +
  geom_histogram(aes(x = phq9_tot)) + 
  theme_bw()

p8 <- ggplot(raw_data) +
  geom_histogram(aes(x = age_BLchart)) + 
  theme_bw()

p9 <- ggplot(raw_data) +
  geom_histogram(aes(x = ace_score)) + 
  theme_bw()

grid.arrange(p3, p4, p5, p6, p7, p8, ncol = 2)
```

### Table A1. Processed Variable Summary

```{r, message = FALSE}
tbl_summary(tb_df)#%>%
   #as_kable_extra(booktabs = TRUE, 
                # longtable = TRUE) #%>%
 # kableExtra::kable_styling(font_size = 9, 
                           # latex_options = c("repeat_header", "HOLD_position"))

```


```{r, eval = FALSE, message = FALSE}


# load risk model files
X <- tb_matrix[, -ncol(tb_matrix)]
y <- tb_matrix[, ncol(tb_matrix)]

coef_vals <- matrix(0, ncol=3, nrow=ncol(X)-1)

# risk model prediction 
risk_output_cv <- cv_risk_mod(X, y, a=-5, b=5)
risk_output <- risk_mod(X, y, a=-5, b=5, lambda0=risk_output_cv$lambda_min)
coef_vals[,1] <- risk_output$beta[-1]
risk_probs <- predict(risk_output$glm_mod, type="response")
risk_pred <- as.factor(ifelse(risk_probs < 0.5, FALSE, TRUE))

# glm prediction
glm_mod <- glm(y~X-1, family = "binomial")
coef_vals[, 2] <- coef(glm_mod)[-1]
glm_probs<- predict(glm_mod, type="response")
glm_pred <- as.factor(ifelse(glm_probs < 0.5, FALSE, TRUE))

# lasso prediction

lasso_res <- cv.glmnet(x=X[,-1], y=y, alpha=1)
lasso_mod <- glmnet(x=X[,-1], y=y, lambda=lasso_res$lambda.min, alpha=1)
coef_vals[,3] <- coef(lasso_mod)[-1]
lasso_probs <- as.vector(predict(lasso_mod, newx=X[,-1]))
lasso_pred <- as.factor(ifelse(lasso_probs < 0.5, FALSE, TRUE))

print(coef_vals)

# confusion matrices
confusionMatrix(as.factor(tb_df$adherence_outcome), risk_pred)
confusionMatrix(as.factor(tb_df$adherence_outcome), glm_pred)
confusionMatrix(as.factor(tb_df$adherence_outcome), lasso_pred)

# discrimination
roc(as.factor(tb_df$adherence_outcome), risk_probs)
roc(as.factor(tb_df$adherence_outcome), glm_probs)
roc(as.factor(tb_df$adherence_outcome), lasso_probs)

# find risk score probs to summarize
tb_df$scores <- (X[,-1] %*% risk_output$beta[-1])
ggplot(tb_df)+geom_histogram(aes(x=scores, fill=adherence_outcome), alpha=0.5)

# for each score find predicted probability and also find percent class 1
range_scores <- range(tb_df$scores)
all_scores <- seq(range_scores[1], range_scores[2])
vals <- risk_output$gamma*(risk_output$beta[1]+range_scores)
probs <- exp(vals)/(1+exp(vals))
props <- tb_df %>% 
  mutate(rnd_scores = floor(scores)) %>%
  group_by(rnd_scores) %>% 
  summarize(prop = sum(adherence_outcome)/n()) 

ggplot()+geom_line(aes(x=range_scores, y=probs)) + 
  geom_point(aes(x=props$rnd_scores,y=props$prop)) + 
  labs(x="Score", y="Predicted or Observed Probability")



```


\pagebreak

# Code Appendix



```{r  ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, include=TRUE}

```


