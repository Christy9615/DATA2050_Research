e4 <- ivreg(obs.Y ~ A|Z)
e4 <- coef(e4)[2]
output <- c(e1,e2,e3,e4)
estimator_list <- rbind(estimator_list,output)
}
estimator_list <- as.data.frame(estimator_list)
View(estimator_list)
names(estimator_list) <- c('e1','e2','e3','e4')
View(estimator_list)
library(tidyverse)
estimator_list %>% pivot_longer(names_to = 'estimator', values_to = 'values')
estimator_list %>% pivot_longer(names_to = 'estimator', values_to = 'values')
estimator_list %>% pivot_longer(c('e1','e2','e3','e4'),names_to = 'estimator', values_to = 'values')
estimator_list %>% pivot_longer(c('e1','e2','e3','e4'),names_to = 'estimator', values_to = 'values') %>%
group_by(estimator) %>% summarise(bias = mean(values-1))
bias_dat <- estimator_list %>% pivot_longer(c('e1','e2','e3','e4'),names_to = 'estimator', values_to = 'values') %>%
group_by(estimator) %>% summarise(bias = mean(values-1))
bias_dat
# Q4
sicily <- read.csv('sicily.csv')
View(sicily)
# Q4
# load the data
data <- read.csv ("sicily.csv")
# compute the standardized rates
data$rate <- with(data,aces/stdpop * 10^5)
View(data)
stdpop
# compute the standardized rates
data$rate <- with(data,aces/stdpop * 10^5)
# fit a Poisson with the standardised population as an offset
model1 <- glm(aces~offset(log(stdpop)) + smokban + time , family = poisson ,data)
predict(model1)
predict(model1,time=5)
# fit a Poisson with the standardised population as an offset
model1 <- glm(aces~offset(log(stdpop)) + smokban + time , family = poisson ,data)
model1
data[36,]
rbind(c(data[36,],data[36,]))
data[36,]
# fit a Poisson with the standardised population as an offset
new_data <- rbind(data[36,],data[36,])
View(new_data)
new_data[2,]
new_data[2,5]
new_data[2,5] <- 1
new_data
predict.glm(model1,newdata = new_data)
model1 <- glm(aces~offset(log(stdpop)) + smokban + time , family = poisson ,data=data)
predict.glm(model1,newdata = new_data)
coefficients( model1 )
confint( model1 )
exp(6.791777 )
exp(predict.glm(model1,newdata = new_data))
# Q4.5
# fit a Poisson with the standardised population as an offset
model2 <- glm (aces ~ offset (log(stdpop)) + smokban*time,
family = poisson , data)
family = poisson , data)
coef(model2)
data
new_data <- rbind(data[37,],data[37,])
new_data[2,5] <- 1
new_data
new_data[1,5] <- 0
new_data
predict.glm(model2,newdata = new_data)
predict.glm(model2,newdata = new_data)[1] - predict.glm(model2,newdata = new_data)[2]
predict.glm(model2,newdata = new_data)[1]
# Year 2006 Month 1
new_data <- rbind(data[49,],data[49,])
new_data[1,5] <- 0
predict.glm(model2,newdata = new_data)[1] - predict.glm(model2,newdata = new_data)[2]
library(wooldridge)
library(WeightIt)
library(survey)
library(tidyverse)
data(card)
## Q1.1
var_list_predict = c('exper', 'expersq', 'black', 'smsa', 'south', 'smsa66', 'reg662', 'reg663', 'reg664', 'reg665', 'reg666',
'reg667', 'reg668', 'reg669')
ols <- lm(lwage~educ+exper+expersq+black+smsa+south+smsa66+reg662+reg663+reg664+reg665+reg666+reg667+reg668+reg669, data = card)
coef(summary(ols))
## Q1.2
s1_mod <- lm(educ~nearc4+exper+expersq+black+smsa+south+smsa66+reg662+reg663+reg664+reg665+reg666+reg667+reg668+reg669, data = card )
coef(summary(s1_mod))
card$educ_pred <- predict(s1_mod)
## Q1.3
s2_mod <- lm(lwage~educ_pred+exper+expersq+black+smsa+south+smsa66+reg662+reg663+reg664+reg665+reg666+reg667+reg668+reg669, data = card)
coef(summary(s2_mod))
## Q1.4
library(ivreg)
iv_mod <- ivreg(lwage~educ|nearc4 +
exper+expersq+black+smsa+south+smsa66+reg662+reg663+reg664+reg665+reg666+reg667+reg668+reg669,data = card)
coef(iv_mod)
## Q1.4
library(ivreg)
iv_mod <- ivreg(lwage~educ+exper+expersq+black+smsa+south+smsa66+reg662+reg663+reg664+reg665+reg666+reg667+reg668+reg669|nearc4 +
exper+expersq+black+smsa+south+smsa66+reg662+reg663+reg664+reg665+reg666+reg667+reg668+reg669,data = card)
coef(iv_mod)
# Q2.3
estimator_list = c()
for (r in 1:1000) {
set.seed(r)
n = 10000
Z = rbinom (n, 1, 0.4)
X = rnorm (n, 0, 1)
U = rnorm (n, 0, 1)
A = rbinom (n, 1, plogis (-1 + Z + 0.5*X - 0.3*U))
Y0 = rnorm (n, X + 0.5*U, 0.5)
Y1 = rnorm (n, Y0 + 1, 0.5)
obs.Y = ifelse (A == 1, Y1 , Y0)
dat = data.frame (obs.Y = obs.Y, A = A, Z = Z, X = X)
e1 <- mean(obs.Y[A==1]) - mean(obs.Y[A==0])
e2.out <- weightit(A~X, estimand = "ATE",
method = "ps",data = dat)
design.e2 <- svydesign(~1, weights = e2.out$weights,data=dat)
fit.e2 <- svyglm(obs.Y ~ A+X,
design = design.e2)
e2 <- coef(fit.e2)[2]
e3 <- (mean(obs.Y[Z==1]) - mean(obs.Y[Z==0])) / (mean(A[Z==1]) - mean(A[Z==0]))
e4 <- ivreg(obs.Y ~ A|Z)
e4 <- coef(e4)[2]
output <- c(e1,e2,e3,e4)
estimator_list <- rbind(estimator_list,output)
}
estimator_list <- as.data.frame(estimator_list)
names(estimator_list) <- c('e1','e2','e3','e4')
bias_dat <- estimator_list %>% pivot_longer(c('e1','e2','e3','e4'),names_to = 'estimator', values_to = 'values') %>%
group_by(estimator) %>% summarise(bias = mean(values-1))
bias_dat
library(MASS)
sim_dat <- function(n,p,s,beta_type,rho,snr){
# Generate Beta values based on beta_types
B.zero <- rep(0, p)
# Beta_type conditions
if(beta_type==1) {
indices =  step <- p %/% (s + 1)
for (i in 1:s) {
index <- i * step
B.zero[index] <- 1
B.zero[is.na(B.zero)] <- 0}
} else if (beta_type==2) {
B.zero <- c(rep(1,s),rep(0,p-s))
} else if (beta_type==3 | beta_type==4) {
B.zero <- c(seq(10, 0.5, length.out = s),rep(0,p-s))
} else {
B.zero[1:s] <- 1
for (i in (s+1) : p ) {
B.zero[i] = 0.5 ** (i-s)
}
}
# Generate X matrix
# Generate X matrix from multivariate Gaussian with exponential correlation
cov_mat <- matrix(0,nrow = p,ncol = p)
for (row in 1:p) {
for (col in 1:p) {
cov_mat[row, col] = rho ** abs(row-col)
}
}
# generate X
X <- mvrnorm(n=n, mu=rep(0,p), Sigma = cov_mat)
# Generate Y
sigma.sq <- (t(as.matrix(B.zero)) %*% cov_mat %*% as.matrix(B.zero)) / snr
cov_mat_y <- sigma.sq[1,1] * diag(1,n,n)
Y <- mvrnorm(n = 1, mu = X %*% as.matrix(B.zero), Sigma = cov_mat_y)
# Return list of X and Y
return(list(X=X, y=Y))
}
#sim_dat(50,6,3,5,0.5,2)
train_sim <- sim_dat(n=100,p=10,s=5,beta_type=2,rho=0.35,snr=2.07)
train <- as.data.frame(cbind(train_sim$X,
train_sim$y))
names(train) <- c('X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','Y')
train_sim <- sim_dat(n=100,p=10,s=5,beta_type=2,rho=0.35,snr=2.07)
train <- as.data.frame(cbind(train_sim$X,
train_sim$y))
names(train) <- c('X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','Y')
k <- 10
n <- nrow(train)
set.seed (1)
folds <- sample(rep (1:k, length = n))
cv.errors <- matrix(NA, k, 10,dimnames = list(NULL , paste (1:10)))
predict.regsubsets <- function(best.fit, newdata, id) {
object <- best.fit
form <- as.formula(object$call [[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object , id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
k <- 10
n <- nrow(train)
set.seed (1)
folds <- sample(rep (1:k, length = n))
cv.errors <- matrix(NA, k, 10,dimnames = list(NULL , paste (1:10)))
predict.regsubsets <- function(best.fit, newdata, id) {
object <- best.fit
form <- as.formula(object$call [[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object , id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
for (j in 1:k) {
best.fit <- regsubsets(Y ~ .,data = train[folds != j, ],nvmax = 10)
for (i in 1:10) {
pred <- predict.regsubsets(best.fit , train[folds == j, ], id = i)
cv.errors[j, i] <- mean (( train$Y[folds == j] - pred)^2)
}
}
mean.cv.errors <- apply(cv.errors , 2, mean)
min(mean.cv.errors)
library(leaps)
train_sim <- sim_dat(n=100,p=10,s=5,beta_type=2,rho=0.35,snr=2.07)
train <- as.data.frame(cbind(train_sim$X,
train_sim$y))
names(train) <- c('X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','Y')
train
k <- 10
n <- nrow(train)
set.seed (2550)
folds <- sample(rep (1:k, length = n))
cv.errors <- matrix(NA, k, 10,dimnames = list(NULL , paste (1:10)))
cv.errors
train_sim <- sim_dat(n=100,p=10,s=5,beta_type=2,rho=0.35,snr=0.5)
train <- as.data.frame(cbind(train_sim$X,
train_sim$y))
names(train) <- c('X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','Y')
k <- 10
n <- nrow(train)
set.seed (2550)
folds <- sample(rep (1:k, length = n))
cv.errors <- matrix(NA, k, 10,dimnames = list(NULL , paste (1:10)))
predict.regsubsets <- function(best.fit, newdata, id) {
object <- best.fit
form <- as.formula(object$call [[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object , id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
for (j in 1:k) {
best.fit <- regsubsets(Y ~ .,data = train[folds != j, ],nvmax = 10)
for (i in 1:10) {
pred <- predict.regsubsets(best.fit , train[folds == j, ], id = i)
cv.errors[j, i] <- mean (( train$Y[folds == j] - pred)^2)
}
}
mean.cv.errors <- apply(cv.errors , 2, mean)
min(mean.cv.errors)
mean.cv.errors
setwd("~/Documents/GitHub/Risk_Model_Research/MILP")
library(ompr) # Package for establish OMPR Model
library(ompr.roi) # Solver package
library(ROI.plugin.glpk) # Specific solver used
library(tidyverse)
# Read in Data for illustration
files <- list.files("/Users/oscar/Documents/GitHub/Risk_Model_Research/MILP/data")
# Read in Data for illustration
files <- list.files("/Users/oscar/Documents/GitHub/Risk_Model_Research/MILP/data")
files
df <- read.csv(paste0("/Users/oscar/Documents/GitHub/Risk_Model_Research/MILP/data/",files[1]))
View(df)
max_capacity <- 5
n <- 10
set.seed(1234)
weights <- runif(n, max = max_capacity)
MIPModel()  %>%
add_variable(x[i], i = 1:n, type = "binary") %>%
set_objective(sum_over(weights[i] * x[i], i = 1:n), "max") %>%
add_constraint(sum_over(weights[i] * x[i], i = 1:n) <= max_capacity) %>%
solve_model(with_ROI(solver = "glpk")) %>%
get_solution(x[i]) %>%
filter(value > 0)
# Set dat matrix
y <- df[[1]]
X <- as.matrix(df[,2:ncol(df)])
# MILP Version 2
n = nrow(X)
p = ncol(X[,-1])
M = Inf
X[,5]
Lambda0 = 0
# Score Pool
K = c(0,seq(1:10))
PI = sort(runif(100))
K
PI
MIPModel() %>%
# Integer coefficients for attributes
add_variable(beta[j], j=1:p, type = 'integer') %>%
# Predicted Risk Score from the integer coefficients
add_variable(s[i], i= 1:n, type = 'integer',lb=0,up=10) %>%
add_constraint(sum_over(beta[j]*X[i,j], j=1:p) == s[i], i=1:n) %>%
# True Risk Score, set bounds
# add_variable(k[a],a=1:n,type = 'integer',lb=0,up=10) %>%
# Indicator of S_i = k, k: one potential score from the score pool
add_variable(z_ik[i,k], i=1:n, k=1:length(K), type = 'binary') %>%
add_constraint(sum_expr(z_ik[i,k], k = 1:length(K)) == 1, i=1:n) %>%
add_constraint(s[i] - K[k] <= M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
add_constraint(s[i] - K[k] >= -M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
# Indicator of score k assigned to prob pi_l
add_variable(z_kl[k,l], k=1:length(K), l=1:length(PI), type = 'binary') %>%
# each k is asigned to exactly 1 probs
add_constraint(sum_expr(z_kl[k,l], l=1:length(PI)) == 1,k=1:length(K)) %>%
# higher k is associated with higher probs ???
#add_constraint(z_kl[k,l] <= 1 - z_kl[k-1,l_star], l<l_star, l=1:length(PI),l_star=1:length(PI),k=1:length(K)) %>%
# Indicator of point i assigned with probs pi_l
add_variable(pr[i,l], i=1:n, l=1:length(PI),type = 'binary') %>%
add_constraint(pr[i,l] >= z_kl[k,l] + z_ik[i,k] - 1, i=1:n,k=1:length(K),l=1:length(PI)) %>%
# Each i have exactly one probs
add_constraint(sum_expr(pr[i,l], l=1:length(PI)) == 1, i=1:n) %>%
# Penalty Expression
add_variable(lambda[j],j=1:p,type = 'binary') %>%
add_constraint(beta[j] <= M*lambda[j], j=1:p) %>%
add_constraint(beta[j] <= -M*lambda[j], j=1:p) %>%
# Objective Function
set_objective(-sum_expr(y[i] * log(PI[l]) * pr[i,l] + (1-y[i]) * log(1-PI[l]) * pr[i,l], i=1:n,l=1:length(PI))
+ Lambda0 * sum_expr(lambda[j],j=1:p) , sense = "min")
p
p = ncol(X)
# Q4
# load the data
data <- read.csv ("sicily.csv")
# compute the standardized rates
data$rate <- with(data,aces/stdpop * 10^5)
# fit a Poisson with the standardised population as an offset
new_data <- rbind(data[36,],data[36,])
# Q4
# load the data
data <- read.csv ("sicily.csv")
setwd("~/Google Drive/My Drive/Brown/Fall 2023/PHP 2610/Problem Set")
library(wooldridge)
library(WeightIt)
library(survey)
library(tidyverse)
# Q4
# load the data
data <- read.csv ("sicily.csv")
# compute the standardized rates
data$rate <- with(data,aces/stdpop * 10^5)
# fit a Poisson with the standardised population as an offset
new_data <- rbind(data[36,],data[36,])
new_data[2,5] <- 1
model1 <- glm(aces~offset(log(stdpop)) + smokban + time , family = poisson ,data=data)
exp(predict.glm(model1,newdata = new_data))
new_data[2,5]
new_data
setwd("~/Documents/GitHub/Risk_Model_Research/MILP")
k=1:length(K)
length(K)
c(0,seq(1:10))
MIPModel() %>%
# Integer coefficients for attributes
add_variable(beta[j], j=1:p, type = 'integer') %>%
# Predicted Risk Score from the integer coefficients : ??? should we put constraints
add_variable(s[i], i= 1:n, type = 'integer') %>%
add_constraint(sum_expr(beta[j]*X[i,j], j=1:p) == s[i], i=1:n) %>%
# True Risk Score, set bounds
# add_variable(k[a],a=1:n,type = 'integer',lb=0,up=10) %>%
# Indicator of S_i = k, k: one potential score from the score pool
add_variable(z_ik[i,k], i=1:n, k=1:length(K), type = 'binary') %>%
add_constraint(sum_expr(z_ik[i,k], k = 1:length(K)) == 1, i=1:n) %>%
add_constraint(s[i] - K[k] <= M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
add_constraint(s[i] - K[k] >= -M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
# Indicator of score k assigned to prob pi_l
add_variable(z_kl[k,l], k=1:length(K), l=1:length(PI), type = 'binary') %>%
# each k is asigened to exactly 1 probs
add_constraint(sum_expr(z_kl[k,l], l=1:length(PI)) == 1,k=1:length(K)) %>%
# higher k is associated with higher probs ???
#add_constraint(z_kl[k,l] <= 1 - z_kl[k-1,l_star], l<l_star, l=1:length(PI),l_star=1:length(PI),k=1:length(K)) %>%
# Indicator of point i assigned with probs pi_l
add_variable(pr[i,l], i=1:n, l=1:length(PI),type = 'binary') %>%
add_constraint(pr[i,l] >= z_kl[k,l] + z_ik[i,k] - 1, i=1:n,k=1:length(K),l=1:length(PI)) %>%
# Each i have exactly one probs
add_constraint(sum_expr(pr[i,l], l=1:length(PI)) == 1, i=1:n) %>%
# Penalty Expression
add_variable(lambda[j],j=1:p,type = 'binary') %>%
add_constraint(beta[j] <= M*lambda[j], j=1:p) %>%
add_constraint(beta[j] <= -M*lambda[j], j=1:p) %>%
# Objective Function
set_objective(-sum_expr(y[i] * log(PI[l]) * pr[i,l] + (1-y[i]) * log(1-PI[l]) * pr[i,l], i=1:n,l=1:length(PI))
+ Lambda0 * sum_expr(lambda[j],j=1:p) , sense = "min") %>%
solve_model(with_ROI(solver = "glpk")) %>%
get_solution()
MIPModel() %>%
# Integer coefficients for attributes
add_variable(beta[j], j=1:p, type = 'integer') %>%
# Predicted Risk Score from the integer coefficients : ??? should we put constraints
add_variable(s[i], i= 1:n, type = 'integer') %>%
add_constraint(sum_expr(beta[j]*X[i,j], j=1:p) == s[i], i=1:n) %>%
# True Risk Score, set bounds
# add_variable(k[a],a=1:n,type = 'integer',lb=0,up=10) %>%
# Indicator of S_i = k, k: one potential score from the score pool
add_variable(z_ik[i,k], i=1:n, k=1:length(K), type = 'binary') %>%
add_constraint(sum_expr(z_ik[i,k], k = 1:length(K)) == 1, i=1:n) %>%
add_constraint(s[i] - K[k] <= M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
add_constraint(s[i] - K[k] >= -M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
# Indicator of score k assigned to prob pi_l
add_variable(z_kl[k,l], k=1:length(K), l=1:length(PI), type = 'binary') %>%
# each k is asigened to exactly 1 probs
add_constraint(sum_expr(z_kl[k,l], l=1:length(PI)) == 1,k=1:length(K)) %>%
# higher k is associated with higher probs ???
#add_constraint(z_kl[k,l] <= 1 - z_kl[k-1,l_star], l<l_star, l=1:length(PI),l_star=1:length(PI),k=1:length(K)) %>%
# Indicator of point i assigned with probs pi_l
add_variable(pr[i,l], i=1:n, l=1:length(PI),type = 'binary') %>%
add_constraint(pr[i,l] >= z_kl[k,l] + z_ik[i,k] - 1, i=1:n,k=1:length(K),l=1:length(PI)) %>%
# Each i have exactly one probs
add_constraint(sum_expr(pr[i,l], l=1:length(PI)) == 1, i=1:n) %>%
# Penalty Expression
add_variable(lambda[j],j=1:p,type = 'binary') %>%
add_constraint(beta[j] <= M*lambda[j], j=1:p) %>%
add_constraint(beta[j] <= -M*lambda[j], j=1:p) %>%
# Objective Function
set_objective(-sum_expr(y[i] * log(PI[l]) * pr[i,l] + (1-y[i]) * log(1-PI[l]) * pr[i,l], i=1:n,l=1:length(PI))
+ Lambda0 * sum_expr(lambda[j],j=1:p) , sense = "min")
1:length(K)
1:length(PI)
MIPModel() %>%
# Integer coefficients for attributes
add_variable(beta[j], j=1:p, type = 'integer') %>%
# Predicted Risk Score from the integer coefficients : ??? should we put constraints
add_variable(s[i], i= 1:n, type = 'integer') %>%
add_constraint(sum_expr(beta[j]*X[i,j], j=1:p) == s[i], i=1:n) %>%
# True Risk Score, set bounds
# add_variable(k[a],a=1:n,type = 'integer',lb=0,up=10) %>%
# Indicator of S_i = k, k: one potential score from the score pool
add_variable(z_ik[i,k], i=1:n, k=1:length(K), type = 'binary') %>%
add_constraint(sum_expr(z_ik[i,k], k = 1:length(K)) == 1, i=1:n) %>%
add_constraint(s[i] - K[k] <= M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
add_constraint(s[i] - K[k] >= -M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
# Indicator of score k assigned to prob pi_l
add_variable(z_kl[k,l], k=1:length(K), l=1:length(PI), type = 'binary') %>%
# each k is asigened to exactly 1 probs
add_constraint(sum_expr(z_kl[k,l], l=1:length(PI)) == 1,k=1:length(K)) %>%
# higher k is associated with higher probs ???
add_constraint(z_kl[k,l] <= 1 - z_kl[k-1,l_star], l<l_star, l=1:length(PI),l_star=1:length(PI),k=1:length(K)) %>%
# Indicator of point i assigned with probs pi_l
add_variable(pr[i,l], i=1:n, l=1:length(PI),type = 'binary') %>%
add_constraint(pr[i,l] >= z_kl[k,l] + z_ik[i,k] - 1, i=1:n,k=1:length(K),l=1:length(PI)) %>%
# Each i have exactly one probs
add_constraint(sum_expr(pr[i,l], l=1:length(PI)) == 1, i=1:n) %>%
# Penalty Expression
add_variable(lambda[j],j=1:p,type = 'binary') %>%
add_constraint(beta[j] <= M*lambda[j], j=1:p) %>%
add_constraint(beta[j] <= -M*lambda[j], j=1:p) %>%
# Objective Function
set_objective(-sum_expr(y[i] * log(PI[l]) * pr[i,l] + (1-y[i]) * log(1-PI[l]) * pr[i,l], i=1:n,l=1:length(PI))
+ Lambda0 * sum_expr(lambda[j],j=1:p) , sense = "min")
MIPModel() %>%
# Integer coefficients for attributes
add_variable(beta[j], j=1:p, type = 'integer') %>%
# Predicted Risk Score from the integer coefficients : ??? should we put constraints
add_variable(s[i], i= 1:n, type = 'integer') %>%
add_constraint(sum_expr(beta[j]*X[i,j], j=1:p) == s[i], i=1:n) %>%
# True Risk Score, set bounds
# add_variable(k[a],a=1:n,type = 'integer',lb=0,up=10) %>%
# Indicator of S_i = k, k: one potential score from the score pool
add_variable(z_ik[i,k], i=1:n, k=1:length(K), type = 'binary') %>%
add_constraint(sum_expr(z_ik[i,k], k = 1:length(K)) == 1, i=1:n) %>%
add_constraint(s[i] - K[k] <= M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
add_constraint(s[i] - K[k] >= -M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
# Indicator of score k assigned to prob pi_l
add_variable(z_kl[k,l], k=1:length(K), l=1:length(PI), type = 'binary') %>%
# each k is asigened to exactly 1 probs
add_constraint(sum_expr(z_kl[k,l], l=1:length(PI)) == 1,k=1:length(K)) %>%
# higher k is associated with higher probs ???
add_constraint(z_kl[k,l] <= 1 - z_kl[k-1,l_star],  l=1:length(PI),l_star=1:length(PI),k=1:length(K)) %>%
# Indicator of point i assigned with probs pi_l
add_variable(pr[i,l], i=1:n, l=1:length(PI),type = 'binary') %>%
add_constraint(pr[i,l] >= z_kl[k,l] + z_ik[i,k] - 1, i=1:n,k=1:length(K),l=1:length(PI)) %>%
# Each i have exactly one probs
add_constraint(sum_expr(pr[i,l], l=1:length(PI)) == 1, i=1:n) %>%
# Penalty Expression
add_variable(lambda[j],j=1:p,type = 'binary') %>%
add_constraint(beta[j] <= M*lambda[j], j=1:p) %>%
add_constraint(beta[j] <= -M*lambda[j], j=1:p) %>%
# Objective Function
set_objective(-sum_expr(y[i] * log(PI[l]) * pr[i,l] + (1-y[i]) * log(1-PI[l]) * pr[i,l], i=1:n,l=1:length(PI))
+ Lambda0 * sum_expr(lambda[j],j=1:p) , sense = "min")
MIPModel() %>%
# Integer coefficients for attributes
add_variable(beta[j], j=1:p, type = 'integer') %>%
# Predicted Risk Score from the integer coefficients : ??? should we put constraints
add_variable(s[i], i= 1:n, type = 'integer') %>%
add_constraint(sum_expr(beta[j]*X[i,j], j=1:p) == s[i], i=1:n) %>%
# True Risk Score, set bounds
# add_variable(k[a],a=1:n,type = 'integer',lb=0,up=10) %>%
# Indicator of S_i = k, k: one potential score from the score pool
add_variable(z_ik[i,k], i=1:n, k=1:length(K), type = 'binary') %>%
add_constraint(sum_expr(z_ik[i,k], k = 1:length(K)) == 1, i=1:n) %>%
add_constraint(s[i] - K[k] <= M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
add_constraint(s[i] - K[k] >= -M * (1-z_ik[i,k]), i=1:n,k=1:length(K)) %>%
# Indicator of score k assigned to prob pi_l
add_variable(z_kl[k,l], k=1:length(K), l=1:length(PI), type = 'binary') %>%
# each k is asigened to exactly 1 probs
add_constraint(sum_expr(z_kl[k,l], l=1:length(PI)) == 1,k=1:length(K)) %>%
# higher k is associated with higher probs ???
#add_constraint(z_kl[k,l] <= 1 - z_kl[k-1,l_star], l=1:length(PI),l_star=1:length(PI),k=1:length(K)) %>%
# Indicator of point i assigned with probs pi_l
add_variable(pr[i,l], i=1:n, l=1:length(PI),type = 'binary') %>%
add_constraint(pr[i,l] >= z_kl[k,l] + z_ik[i,k] - 1, i=1:n,k=1:length(K),l=1:length(PI)) %>%
# Each i have exactly one probs
add_constraint(sum_expr(pr[i,l], l=1:length(PI)) == 1, i=1:n) %>%
# Penalty Expression
add_variable(lambda[j],j=1:p,type = 'binary') %>%
add_constraint(beta[j] <= M*lambda[j], j=1:p) %>%
add_constraint(beta[j] <= -M*lambda[j], j=1:p) %>%
# Objective Function
set_objective(-sum_expr(y[i] * log(PI[l]) * pr[i,l] + (1-y[i]) * log(1-PI[l]) * pr[i,l], i=1:n,l=1:length(PI))
+ Lambda0 * sum_expr(lambda[j],j=1:p) , sense = "min")
